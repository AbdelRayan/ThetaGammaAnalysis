{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb597a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: networkx>=2.8 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (3.1)\n",
      "Requirement already satisfied: packaging>=21 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (0.3)\n",
      "Requirement already satisfied: pillow>=9.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (10.0.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.27 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2.31.3)\n",
      "Requirement already satisfied: scipy>=1.8 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (1.11.2)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-image) (2023.8.30)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=21->scikit-image) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\janlu\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49dca46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emd in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.2)\n",
      "Requirement already satisfied: sparse in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (0.14.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (3.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (1.11.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (1.3.2)\n",
      "Requirement already satisfied: dcor in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (0.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (6.0.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from emd) (2.1.0)\n",
      "Requirement already satisfied: numba>=0.51 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from dcor->emd) (0.57.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (4.42.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (1.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (10.0.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (0.11.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->emd) (21.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->emd) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->emd) (2023.3)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from numba>=0.51->dcor->emd) (0.40.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->emd) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\janlu\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bcc987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pingouin in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (1.3.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.2 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (3.7.2)\n",
      "Requirement already satisfied: outdated in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (0.2.2)\n",
      "Requirement already satisfied: pandas-flavor>=0.2.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (0.6.0)\n",
      "Requirement already satisfied: pandas>=1.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (2.1.0)\n",
      "Requirement already satisfied: statsmodels>=0.13 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (0.14.0)\n",
      "Requirement already satisfied: seaborn>=0.11 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (0.12.2)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (1.11.2)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (1.24.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pingouin) (0.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (4.42.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (10.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib>=3.0.2->pingouin) (1.4.5)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0->pingouin) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas>=1.0->pingouin) (2023.3)\n",
      "Requirement already satisfied: xarray in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas-flavor>=0.2.0->pingouin) (2023.8.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from statsmodels>=0.13->pingouin) (0.5.3)\n",
      "Requirement already satisfied: setuptools>=44 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from outdated->pingouin) (58.1.0)\n",
      "Requirement already satisfied: requests in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from outdated->pingouin) (2.31.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from outdated->pingouin) (0.2.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->pingouin) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->pingouin) (1.3.2)\n",
      "Requirement already satisfied: six in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from patsy>=0.5.2->statsmodels>=0.13->pingouin) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->outdated->pingouin) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->outdated->pingouin) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->outdated->pingouin) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->outdated->pingouin) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\janlu\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2f0409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from h5py) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\janlu\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c979eb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sails in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (1.24.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (3.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (1.11.2)\n",
      "Requirement already satisfied: importlib-metadata<5.0.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (4.13.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sails) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata<5.0.0->sails) (3.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (2.8.2)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (10.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (1.4.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (1.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->sails) (4.42.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sails) (3.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn->sails) (1.3.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\janlu\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->sails) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\janlu\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install sails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8f2cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emd.sift as sift\n",
    "import emd.spectra as spectra\n",
    "import numpy as np\n",
    "import pingouin as pg\n",
    "import sails\n",
    "import scipy.io as sio\n",
    "import h5py\n",
    "import time\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.stats import zscore, binned_statistic\n",
    "from scipy.ndimage import center_of_mass\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import concurrent.futures\n",
    "from skimage.feature import peak_local_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e1eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The get_rem_states function takes in an array of sleep states and the sample rate of the data.\n",
    "def get_rem_states(states, sample_rate):\n",
    "    \"\"\"\n",
    "    Extract consecutive REM (Rapid Eye Movement) sleep states and their start\n",
    "    and end times from an array of sleep states.\n",
    "\n",
    "    Parameters:\n",
    "    - states (numpy.ndarray): One-dimensional array of sleep states.\n",
    "    - sample_rate (int): The sample rate of the data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: An array containing start and end times of consecutive REM\n",
    "    sleep states. Each row represents a pair of start and end times.\n",
    "\n",
    "    Note:\n",
    "    - Sleep states are represented numerically. In this function, REM sleep\n",
    "      states are identified by the value 5 in the 'states' array.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    sleep_states = np.array([1, 2, 5, 5, 5, 3, 2, 5, 5, 4, 1])\n",
    "    sample_rate = 2500  # Example sample rate in Hz\n",
    "    rem_states_times = get_rem_states(sleep_states, sample_rate)\n",
    "    print(rem_states_times)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the sleep states array is one-dimensional.\n",
    "        states = np.squeeze(states)\n",
    "        # Find the indices where the sleep state is equal to 5, indicating REM sleep.\n",
    "        rem_state_indices = np.where(states == 5)[0]\n",
    "        \n",
    "        # Check if there are no REM states. If so, return an empty array.\n",
    "        if len(rem_state_indices) == 0:\n",
    "            return np.array([])\n",
    "        # Calculate the changes between consecutive REM state indices.\n",
    "        rem_state_changes = np.diff(rem_state_indices)\n",
    "        # Find the indices where consecutive REM states are not adjacent.\n",
    "        split_indices = np.where(rem_state_changes != 1)[0] + 1\n",
    "        # Add indices to split consecutive REM states, including the start and end indices.\n",
    "        split_indices = np.concatenate(([0], split_indices, [len(rem_state_indices)]))\n",
    "        # Create an empty array to store start and end times of consecutive REM states.\n",
    "        consecutive_rem_states = np.empty((len(split_indices) - 1, 2))\n",
    "        # Iterate through the split indices to extract start and end times.\n",
    "        for i, (start, end) in enumerate(zip(split_indices, split_indices[1:])):\n",
    "            start = rem_state_indices[start] * int(sample_rate)\n",
    "            end = rem_state_indices[end - 1] * int(sample_rate)\n",
    "            consecutive_rem_states[i] = np.array([start, end])\n",
    "        # Convert the array to a numpy array.\n",
    "        ##consecutive_rem_states = np.array(consecutive_rem_states)\n",
    "        # Create a mask to filter out consecutive REM states with negative duration.\n",
    "        null_states_mask = np.squeeze(np.diff(consecutive_rem_states) > 0)\n",
    "        consecutive_rem_states = consecutive_rem_states[null_states_mask]\n",
    "        # Return the array containing start and end times of consecutive REM states.\n",
    "        return consecutive_rem_states\n",
    "    # Handle the case where an IndexError occurs, typically due to an empty array.\n",
    "    except IndexError as e:\n",
    "        print(f\"An IndexError occurred in get_rem_states: {e}\")\n",
    "        return np.array([])  # or any default value you want\n",
    "\n",
    "\n",
    "# This function computes the Morlet wavelet transform of a given signal.\n",
    "# It uses the SAILS library to perform the wavelet transform.\n",
    "def morlet_wt(x, sample_rate, frequencies=np.arange(1, 200, 1), n=5, mode='complex'):\n",
    "        \"\"\"\n",
    "    Compute the Morlet wavelet transform of a given signal using the SAILS library.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): The input signal.\n",
    "    - sample_rate (int): The rate at which the signal is sampled.\n",
    "    - frequencies (numpy.ndarray, optional): The array of frequencies at which to compute the transform\n",
    "      (default is from 1 to 200 Hz).\n",
    "    - n (int, optional): The number of cycles in the Morlet wavelet (default is 5).\n",
    "    - mode (str, optional): The mode of the return, whether 'complex', 'power', or 'amplitude'\n",
    "      (default is 'complex').\n",
    "      \n",
    "    Returns:\n",
    "    - numpy.ndarray: The computed Morlet wavelet transform of the input signal.\n",
    "\n",
    "    Note:\n",
    "    - This function relies on the SAILS library to perform the wavelet transform.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    signal = np.sin(2 * np.pi * 10 * np.arange(0, 1, 1/sample_rate))\n",
    "    wt_result = morlet_wt(signal, sample_rate)\n",
    "    print(wt_result)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    wavelet_transform = sails.wavelet.morlet(x, freqs=frequencies, sample_rate=sample_rate, ncycles=n,\n",
    "                                             ret_mode=mode, normalise=None)\n",
    "    # Return the computed wavelet transform.\n",
    "    return wavelet_transform\n",
    "\n",
    "\n",
    "# The tg_split function categorizes frequency values into three groups: sub-theta, theta, and supra-theta.\n",
    "def tg_split(mask_freq, theta_range=(5, 12)):\n",
    "    \"\"\"\n",
    "    Categorize frequency values into three groups: sub-theta, theta, and supra-theta.\n",
    "\n",
    "    Parameters:\n",
    "    - mask_freq (numpy.ndarray): An array of frequency values that you want to categorize.\n",
    "    - theta_range (tuple, optional): A range of frequencies considered as the theta band\n",
    "      (default is (5, 12) Hz).\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing three boolean masks representing sub-theta, theta, and supra-theta categories.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    freq_values = np.array([3, 8, 10, 15, 20])\n",
    "    sub_mask, theta_mask, supra_mask = tg_split(freq_values)\n",
    "    print(\"Sub-theta frequencies:\", freq_values[sub_mask])\n",
    "    print(\"Theta frequencies:\", freq_values[theta_mask])\n",
    "    print(\"Supra-theta frequencies:\", freq_values[supra_mask])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Get the lower and upper bounds of the theta range.\n",
    "    lower = np.min(theta_range)\n",
    "    upper = np.max(theta_range)\n",
    "    # Create a boolean mask for frequencies within the theta range.\n",
    "    mask_index = np.logical_and(mask_freq >= lower, mask_freq < upper)\n",
    "    # Create boolean masks for frequencies below and above the theta range.\n",
    "    sub_mask_index = mask_freq < lower\n",
    "    supra_mask_index = mask_freq > upper\n",
    "    # Assign the boolean masks to variables for each category.\n",
    "    sub = sub_mask_index\n",
    "    theta = mask_index\n",
    "    supra = supra_mask_index\n",
    "    # Return the boolean masks for sub-theta, theta, and supra-theta categories.\n",
    "    return sub, theta, supra\n",
    "\n",
    "# This function finds the indices where a signal crosses zero.\n",
    "# x: The input signal.\n",
    "def zero_cross(x):\n",
    "    \"\"\"\n",
    "    Find the indices where a signal crosses zero.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): The input signal.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array containing the indices where the input signal crosses zero.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    signal = np.array([1, -2, 3, -1, 0, 2, -4, 5])\n",
    "    zero_cross_indices = zero_cross(signal)\n",
    "    print(\"Zero-crossing indices:\", zero_cross_indices)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Identify where the signal goes from positive to non-positive (decay).\n",
    "    decay = np.logical_and((x > 0)[1:], ~(x > 0)[:-1]).nonzero()[0]\n",
    "    # Identify where the signal goes from non-positive to positive (rise).\n",
    "    rise = np.logical_and((x <= 0)[1:], ~(x <= 0)[:-1]).nonzero()[0]\n",
    "    # Combine the indices of rise and decay, then sort them with ascending indices.\n",
    "    zero_xs = np.sort(np.append(rise, decay))\n",
    "    # Return the sorted indices where the signal crosses zero.\n",
    "    return zero_xs\n",
    "\n",
    "# This function identifies the zero crossings, peaks, and troughs in a signal.\n",
    "def extrema(x):\n",
    "    \"\"\"\n",
    "    Identify the zero crossings, peaks, and troughs in a signal.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): The input signal.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing three arrays - zero-crossing indices, trough indices, and peak indices.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    signal = np.array([1, -2, 3, -1, 0, 2, -4, 5])\n",
    "    zero_crossings, trough_indices, peak_indices = extrema(signal)\n",
    "    print(\"Zero-crossing indices:\", zero_crossings)\n",
    "    print(\"Trough indices:\", trough_indices)\n",
    "    print(\"Peak indices:\", peak_indices)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Find the indices where the signal crosses zero.\n",
    "    zero_xs = zero_cross(x)\n",
    "    # Initialize empty arrays to store peak and trough indices.\n",
    "    peaks = np.empty((0,)).astype(int)\n",
    "    troughs = np.empty((0,)).astype(int)\n",
    "    # Iterate through pairs of consecutive zero crossings.\n",
    "    for t1, t2 in zip(zero_xs, zero_xs[1:]):\n",
    "        # Find the index of the maximum absolute value in the current segment.\n",
    "        extrema0 = np.argmax(np.abs(x[t1:t2])).astype(int) + t1\n",
    "        # Check if the value at the found index is positive (peak) or non-positive (trough).\n",
    "        if bool(x[extrema0] > 0):\n",
    "            peaks = np.append(peaks, extrema0)\n",
    "        else:\n",
    "            troughs = np.append(troughs, extrema0)\n",
    "    # Return the indices of zero crossings, troughs, and peaks.\n",
    "    return zero_xs, troughs, peaks\n",
    "\n",
    "#The get_cycles_data function generates a nested dictionary containing extracted data and desired metadata of each REM epochs in the input sleep\n",
    "def get_cycles_data(x, rem_states, sample_rate, frequencies, theta_range=(5, 12)):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a nested dictionary containing extracted data and desired metadata of each REM epoch in the input sleep signal.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): The input 1D sleep signal.\n",
    "    - rem_states (numpy.ndarray): A sleep state vector where 5 represents REM sleep and other values indicate non-REM.\n",
    "    - sample_rate (int or float): The sampling rate of the data.\n",
    "    - frequencies (numpy.ndarray): The array of frequencies at which to compute the wavelet transform.\n",
    "    - theta_range (tuple, optional): A tuple defining the theta frequency range (lower, upper).\n",
    "      Default is (5, 12).\n",
    "\n",
    "    Returns:\n",
    "    - dict: A nested dictionary of extracted signal data and signal source metadata for each REM epoch.\n",
    "\n",
    "    Notes:\n",
    "    - The dictionary output structure comes out as below:\n",
    "      |----REM 1\n",
    "       |    |----start-end:\n",
    "       |    |----wavelet_transform:\n",
    "       |    |----IMFs:\n",
    "       |    |----IMF_Frequencies:\n",
    "       |    |----Instantaneous Phases:\n",
    "       |    |----Instantaneous Frequencies:\n",
    "       |    |----Instantaneous Amplitudes:\n",
    "       |    |----Cycles:\n",
    "      |----REM (...)\n",
    "       |    |--------(...)\n",
    "    \"\"\"\n",
    "\n",
    "    # Squeezing dimensions\n",
    "    x = np.squeeze(x)\n",
    "    rem_states = np.squeeze(rem_states)\n",
    "    # Print the shapes of the input arrays (for debugging)\n",
    "    print(x.shape)\n",
    "    print(rem_states.shape)\n",
    "\n",
    "    # Detect REM periods and get start and end times\n",
    "    consecutive_rem_states = get_rem_states(rem_states, sample_rate).astype(int)\n",
    "    # If the consecutive REM states have an extra dimension, squeeze it out\n",
    "    if consecutive_rem_states.ndim == 3:\n",
    "        consecutive_rem_states=np.squeeze(consecutive_rem_states,0)\n",
    "    # Print the shape of the REM states array (for debugging)\n",
    "    print(consecutive_rem_states.shape)\n",
    "\n",
    "    # Initialize variables to store various extracted data\n",
    "    wt_spectrum = []\n",
    "    rem_imf = []\n",
    "    rem_mask_freq = []\n",
    "    instantaneous_phase = []\n",
    "    instantaneous_freq = []\n",
    "    instantaneous_amp = []\n",
    "    sub_theta_sig = np.empty((0,))\n",
    "    theta_peak_sig = np.empty((0,))\n",
    "    cycles = np.empty((0, 5))\n",
    "    rem_dict = {}\n",
    "    sub_dict = rem_dict\n",
    "\n",
    "    # Loop through each REM epoch\n",
    "    for i, rem in enumerate(consecutive_rem_states, start=1):\n",
    "        # Create a sub-dictionary for the current REM epoch\n",
    "        sub_dict.setdefault(f'REM {i}', {})\n",
    "        # Get the start and end indices of the current REM epoch\n",
    "        start = rem[0]\n",
    "        end = rem[1]\n",
    "        # Extract the signal corresponding to the current REM epoch\n",
    "        signal = x[start:end]\n",
    "\n",
    "        # Generate the time-frequency power spectrum using Morlet wavelet transform\n",
    "        wavelet_transform = morlet_wt(signal, sample_rate, frequencies, mode='amplitude')\n",
    "\n",
    "        # Extraction of IMFs and IMF Frequencies for current REM epoch\n",
    "        imf, mask_freq = sift.iterated_mask_sift(signal,\n",
    "                                                 mask_0='zc',\n",
    "                                                 sample_rate=sample_rate,\n",
    "                                                 ret_mask_freq=True)\n",
    "\n",
    "        # Extract Instantaneous Phase, Frequencies and Amplitudes of each IMF for current REM epoch\n",
    "        IP, IF, IA = spectra.frequency_transform(imf, sample_rate, 'nht')\n",
    "\n",
    "        # Identify sub-theta, theta, and supra-theta frequencies using a given mask\n",
    "        sub_theta, theta, _ = tg_split(mask_freq, theta_range)\n",
    "        # Store the results for the current REM epoch in respective lists\n",
    "        wt_spectrum.append(wavelet_transform)\n",
    "        rem_imf.append(imf)\n",
    "        rem_mask_freq.append(mask_freq)\n",
    "        instantaneous_phase.append(IP)\n",
    "        instantaneous_freq.append(IF)\n",
    "        instantaneous_amp.append(IA)\n",
    "\n",
    "        # Generate the theta signal to detect cycles\n",
    "        theta_sig = np.sum(imf.T[theta], axis=0)\n",
    "\n",
    "        # Parse the sub-theta signal of all REM periods into one variable to set amplitude threshold\n",
    "        sub_theta_sig = np.append(sub_theta_sig, np.sum(imf.T[sub_theta], axis=0))\n",
    "\n",
    "        # Generate extrema locations and zero crossing on the generated theta signal\n",
    "        zero_x, trough, peak = extrema(np.sum(imf.T[theta], axis=0))\n",
    "\n",
    "        # Transpose the IMFs (Intrinsic Mode Functions) to align them properly for further analysis\n",
    "        # Each row in the transposed matrix represents the corresponding IMF for all time points\n",
    "        # This is done to ensure that operations like summing across rows are performed along the correct axis\n",
    "        zero_x = np.vstack((zero_x[:-2:2], zero_x[1:-1:2], zero_x[2::2])).T\n",
    "        \n",
    "\n",
    "        #size_adjust = np.min([trough.shape[0], zero_x.shape[0], peak.shape[0]])\n",
    "        #zero_x = zero_x[:size_adjust]\n",
    "        #cycle = np.empty((size_adjust, 5))\n",
    "        #cycle[:, [0, 2, 4]] = zero_x\n",
    "        #if trough[0] < peak[0]:\n",
    "            #cycle[:, 1] = trough[:zero_x.shape[0]]\n",
    "            #cycle[:, 3] = peak[:zero_x.shape[0]]\n",
    "        #else:\n",
    "            #cycle[:, 3] = trough[:zero_x.shape[0]]\n",
    "            #cycle[:, 1] = peak[:zero_x.shape[0]]\n",
    "            \n",
    "        # Calculate the minimum size among the arrays (trough, zero_x, peak)\n",
    "        size_adjust = np.min([trough.shape[0], zero_x.shape[0], peak.shape[0]])\n",
    "        # Check if the size_adjust is greater than 0\n",
    "        if size_adjust > 0:\n",
    "            # Trim zero_x to the size determined by size_adjust\n",
    "            zero_x = zero_x[:size_adjust]\n",
    "            # Initialize an empty array for cycles with the determined size_adjust\n",
    "            cycle = np.empty((size_adjust, 5))\n",
    "            # Fill the columns related to zero crossings in the cycle array\n",
    "            cycle[:, [0, 2, 4]] = zero_x\n",
    "            # Check the relationship between the first points of trough and peak arrays\n",
    "            if trough[0] < peak[0]:\n",
    "                # If trough comes first, fill the respective columns in the cycle array\n",
    "                cycle[:, 1] = trough[:size_adjust]\n",
    "                cycle[:, 3] = peak[:size_adjust]\n",
    "            else:\n",
    "                # If peak comes first, fill the respective columns in the cycle array\n",
    "                cycle[:, 3] = trough[:size_adjust]\n",
    "                cycle[:, 1] = peak[:size_adjust]\n",
    "        else:\n",
    "            # Handle case when all arrays are empty or size_adjust is zero\n",
    "            cycle = np.empty((0, 5))\n",
    "        # Extract broken cycles (where the condition on diff is not met)\n",
    "        broken_cycle = cycle[~np.all(np.diff(cycle, axis=1) > 0, axis=1)]\n",
    "        # Create a mask for broken cycles based on differences between elements\n",
    "        broken_cycle_mask = np.diff(broken_cycle, axis=1) > 0\n",
    "        # Check if the broken cycles follow the specific adjust_condition pattern\n",
    "        adjust_condition = np.all(np.all(broken_cycle_mask[1:] == [True, False, False, True],\n",
    "                                         axis=0) == True)\n",
    "        # Find the locations where the condition on diff is not met, excluding the first and last points\n",
    "        adjust_loc = np.where(np.all(np.diff(cycle, axis=1) > 0, axis=1) == False)[0][1:-1]\n",
    "        # Extract the fixed cycles between broken cycles\n",
    "        fixed_cycle = broken_cycle[1:-1]\n",
    "        # Adjust the fixed cycles based on the adjust_condition\n",
    "        if adjust_condition:\n",
    "            # If yes, adjust the fixed_cycle based on the adjacent cycles\n",
    "            fixed_cycle[:, 1] = cycle[adjust_loc - 1, 1]\n",
    "            fixed_cycle[:, 3] = cycle[adjust_loc + 1, 3]\n",
    "        else:\n",
    "            # If not, adjust the fixed_cycle based on the adjacent cycles in a different order\n",
    "            fixed_cycle[:, 3] = cycle[adjust_loc - 1, 3]\n",
    "            fixed_cycle[:, 1] = cycle[adjust_loc + 1, 1]\n",
    "        # Check if there are cycles to process\n",
    "        if cycle.size > 0:\n",
    "            # Keep only cycles where the condition on differences between elements is met\n",
    "            cycle = cycle[np.all(np.diff(cycle, axis=1) > 0, axis=1)]\n",
    "            # Stack the fixed cycles on top of the existing cycles\n",
    "            cycle = np.vstack((cycle, fixed_cycle))\n",
    "            # Adjust the columns of the cycles based on the relationship between trough and peak\n",
    "            if trough[0] < peak[0]:\n",
    "                # If trough comes first, adjust the columns accordingly\n",
    "                cycle = np.hstack((cycle[:-1, 1:-1], cycle[1:, :2]))\n",
    "            else:\n",
    "                # If peak comes first, adjust the columns accordingly\n",
    "                cycle = np.hstack((cycle[:-1, 3].reshape((-1, 1)), cycle[1:, :-1]))\n",
    "        else:\n",
    "            # Output an empty array if there are no cycles\n",
    "            cycle = np.empty((0, fixed_cycle.shape[1]))\n",
    "\n",
    "\n",
    "        # Create an array of amplitudes at the peaks in the theta signal\n",
    "        theta_peak_sig = np.append(theta_peak_sig, theta_sig[cycle[:, 2].astype(int)])\n",
    "        # Stack the cycles on top of the existing cycles and adjust for the start index\n",
    "        cycles = np.vstack((cycles, cycle + start))\n",
    "\n",
    "    # Set the minimum amplitude threshold for theta peaks based on standard deviation of sub-theta signal\n",
    "    min_peak_amp = 2 * sub_theta_sig.std()\n",
    "    # Create a mask for satisfying the amplitude threshold criteria for theta peaks\n",
    "    peak_mask = theta_peak_sig > min_peak_amp\n",
    "\n",
    "    # Set the frequency threshold and discard and unsatisfactory difference between trough pairs\n",
    "    upper_diff = np.floor(1000 / np.min(theta_range))\n",
    "    lower_diff = np.floor(1000 / np.max(theta_range))\n",
    "    # Create a mask for satisfying the frequency threshold criteria for cycles\n",
    "    diff_mask = np.logical_and(np.diff(cycles[:, [0, -1]], axis=1) * (1000 / sample_rate) > lower_diff,\n",
    "                               np.diff(cycles[:, [0, -1]], axis=1) * (1000 / sample_rate) <= upper_diff)\n",
    "\n",
    "    # Create a boolean mask that satisfies both the frequency and amplitude threshold criteria\n",
    "    extrema_mask = np.logical_and(np.squeeze(diff_mask), peak_mask)\n",
    "\n",
    "    # Pass the boolean mask on the cycles array to discard any unsatisfactory cycles\n",
    "    cycles = cycles[extrema_mask]\n",
    "\n",
    "    # Place outputs in a nested dictionary\n",
    "    for j, rem in enumerate(rem_dict.values()):\n",
    "        rem['start-end'] = consecutive_rem_states[j]\n",
    "        rem['wavelet_transform'] = wt_spectrum[j]\n",
    "        rem['IMFs'] = rem_imf[j]\n",
    "        rem['IMF_Frequencies'] = rem_mask_freq[j]\n",
    "        rem['Instantaneous Phases'] = instantaneous_phase[j]\n",
    "        rem['Instantaneous Frequencies'] = instantaneous_freq[j]\n",
    "        rem['Instantaneous Amplitudes'] = instantaneous_amp[j]\n",
    "        # Create a boolean mask for cycles within the current REM period\n",
    "        cycles_mask = (cycles > consecutive_rem_states[j, 0]) & (cycles < consecutive_rem_states[j, 1])\n",
    "        # Apply the boolean mask to get cycles within the current REM period\n",
    "        cycles_mask = np.all(cycles_mask == True, axis=1)\n",
    "        rem_cycles = cycles[cycles_mask]\n",
    "        # Assign the cycles values to the nested dictionary after converting to integers\n",
    "        rem['Cycles'] = rem_cycles.astype(int)\n",
    "\n",
    "    return rem_dict\n",
    "\n",
    "def bin_tf_to_fpp(x, power, bin_count):\n",
    "    \"\"\"\n",
    "    Bin the frequency power profile (TF representation) into frequency power profiles (FPP).\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): A 1D or 2D array specifying the frequency ranges for binning.\n",
    "      For a 1D array, it represents the start and end indices of the frequency range.\n",
    "      For a 2D array of size (n, 2), each row represents the start and end indices for each binning range.\n",
    "    - power (numpy.ndarray): The power values in the frequency domain.\n",
    "    - bin_count (int): The number of bins to use for binning the frequency power profile.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 2D array representing the binned frequency power profile.\n",
    "      Each row corresponds to the mean power within each bin for the specified frequency range(s).\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the size of x is invalid.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    frequency_ranges = np.array([[5, 10], [15, 20]])  # Define two frequency ranges\n",
    "    power_spectrum = np.random.rand(100, 30)  # Replace with your actual power spectrum\n",
    "    bin_count = 10\n",
    "\n",
    "    # Bin the power spectrum into frequency power profiles\n",
    "    result_fpp = bin_tf_to_fpp(frequency_ranges, power_spectrum, bin_count)\n",
    "    print(result_fpp)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Check if x is a 1D array (dimensionality of 1)\n",
    "    if x.ndim == 1:  # Handle the case when x is of size (2)\n",
    "        # If yes, create bin ranges using the values in x and specified bin count\n",
    "        bin_ranges = np.arange(x[0], x[1], 1)\n",
    "        # Calculate the mean power within each bin using binned_statistic\n",
    "        fpp = binned_statistic(bin_ranges, power[:, x[0]:x[1]], 'mean', bins=bin_count)[0]\n",
    "        # Add an extra dimension to match the desired output shape (row vector)\n",
    "        fpp = np.expand_dims(fpp, axis=0)  # Add an extra dimension to match the desired output shape\n",
    "    elif x.ndim == 2:  # Handle the case when x is of size (n, 2)\n",
    "        # If yes, initialize an empty list to store results for each row in x\n",
    "        fpp = []\n",
    "        # Iterate through each row in x\n",
    "        for i in range(x.shape[0]):\n",
    "            # Create bin ranges using the values in the current row of x and specified bin count\n",
    "            bin_ranges = np.arange(x[i, 0], x[i, 1], 1)\n",
    "            # Calculate the mean power within each bin using binned_statistic\n",
    "            fpp_row = binned_statistic(bin_ranges, power[:, x[i, 0]:x[i, 1]], 'mean', bins=bin_count)[0]\n",
    "            # Append the result for the current row to the list\n",
    "            fpp.append(fpp_row)\n",
    "        # Convert the list of results to a numpy array\n",
    "        fpp = np.array(fpp)\n",
    "    # If x has an invalid size, raise a ValueError\n",
    "    else:\n",
    "        raise ValueError(\"Invalid size for x\")\n",
    "    # Return the final result (frequency power profile)\n",
    "    return fpp\n",
    "\n",
    "\n",
    "def calculate_cog(frequencies, angles, amplitudes, ratio):\n",
    "    \"\"\"\n",
    "    Calculate the center of gravity (COG) of the frequency and phase distributions.\n",
    "\n",
    "    Parameters:\n",
    "    - frequencies (numpy.ndarray): Array of frequency values.\n",
    "    - angles (numpy.ndarray): Array of phase angles (in degrees).\n",
    "    - amplitudes (numpy.ndarray): Array of amplitude values corresponding to frequencies and angles.\n",
    "    - ratio (float): Threshold ratio for identifying significant amplitudes.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 2D array representing the center of gravity (COG) for each dimension.\n",
    "      Each row corresponds to the COG values for a specific dimension (frequency, phase).\n",
    "\n",
    "    Notes:\n",
    "    - The COG is calculated based on the circular mean of angles weighted by significant amplitudes.\n",
    "    - The threshold for significance is determined by comparing amplitudes to the maximum amplitude within a narrow frequency range.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    frequencies = np.arange(1, 10, 1)\n",
    "    angles = np.random.rand(3, 10) * 360  # Replace with your actual phase angles\n",
    "    amplitudes = np.random.rand(3, 10)  # Replace with your actual amplitude values\n",
    "    ratio = 0.5\n",
    "\n",
    "    # Calculate the COG for the given data\n",
    "    cog_result = calculate_cog(frequencies, angles, amplitudes, ratio)\n",
    "    print(cog_result)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Convert angles to radians\n",
    "    angles = np.deg2rad(angles)\n",
    "    # Initialize an empty array for the center of gravity (COG)\n",
    "    cog = np.empty((0, 2))\n",
    "    # Check if amplitudes have 2 dimensions (2D array)\n",
    "    if amplitudes.ndim == 2:\n",
    "        # Calculate the numerator and denominator for frequency COG\n",
    "        numerator = np.sum(frequencies * np.sum(amplitudes, axis=1))\n",
    "        denominator = np.sum(amplitudes)\n",
    "        # Calculate the frequency COG (cog_f)\n",
    "        cog_f = numerator / denominator\n",
    "        # Calculate floor and ceil indices for frequency COG\n",
    "        floor = np.floor(cog_f).astype(int) - frequencies[0]\n",
    "        ceil = np.ceil(cog_f).astype(int) - frequencies[0]\n",
    "        # Create a new frequency power profile (FPP) with values greater than the threshold ratio\n",
    "        new_fpp = np.where(amplitudes >= np.max(amplitudes[[floor, ceil], :]) * ratio, amplitudes, 0)\n",
    "        # Calculate phase COG using circular mean of angles weighted by FPP\n",
    "        cog_ph = np.rad2deg(pg.circ_mean(angles, w=np.sum(new_fpp, axis=0)))\n",
    "        # Create a 2D array for COG (frequency, phase)\n",
    "        cog = np.array([cog_f, cog_ph])\n",
    "    # Check if amplitudes have 3 dimensions (3D array)\n",
    "    elif amplitudes.ndim == 3:\n",
    "        # Initialize arrays to store indices for amplitude COG calculation\n",
    "        indices_to_subset = np.empty((amplitudes.shape[0], 2)).astype(int)\n",
    "        cog = np.empty((amplitudes.shape[0], 2))\n",
    "        # Calculate numerator and denominator for frequency COG\n",
    "        numerator = np.sum(frequencies * np.sum(amplitudes, axis=2), axis=1)\n",
    "        denominator = np.sum(amplitudes, axis=(1, 2))\n",
    "         # Calculate frequency COG for each dimension\n",
    "        cog_f = (numerator / denominator)\n",
    "        # Vectorize floor and ceil functions for efficient array operations\n",
    "        vectorized_floor = np.vectorize(np.floor)\n",
    "        vectorized_ceil = np.vectorize(np.ceil)\n",
    "        # Set floor and ceil indices for each dimension\n",
    "        indices_to_subset[:, 0] = vectorized_floor(cog_f) - frequencies[0]\n",
    "        indices_to_subset[:, 1] = vectorized_ceil(cog_f) - frequencies[0]\n",
    "        # Calculate max amplitudes for each dimension\n",
    "        max_amps = np.max(amplitudes[np.arange(amplitudes.shape[0])[:, np.newaxis], indices_to_subset, :], axis=(1, 2))\n",
    "        print(max_amps.shape)\n",
    "        # Loop through each dimension and calculate phase COG\n",
    "        for i, max_amp in enumerate(max_amps):\n",
    "            # Create a new FPP for the current dimension with values greater than the threshold ratio\n",
    "            new_fpp = np.where(amplitudes[i] >= max_amp * ratio, amplitudes[i], 0)\n",
    "            # Calculate phase COG using circular mean of angles weighted by FPP\n",
    "            cog[i, 1] = np.rad2deg(pg.circ_mean(angles, w=np.sum(new_fpp, axis=0)))\n",
    "        # Set frequency COG values for each dimension\n",
    "        cog[:, 0] = cog_f\n",
    "    # Return the final COG array\n",
    "    return cog\n",
    "\n",
    "\n",
    "def boxcar_smooth(x, boxcar_window):\n",
    "    \"\"\"\n",
    "    Smooth a 1D or 2D array using a boxcar window.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array to be smoothed.\n",
    "    - boxcar_window (int or tuple): Size of the boxcar window for smoothing.\n",
    "      For 1D array, an integer representing the window size.\n",
    "      For 2D array, a tuple (t, f) representing window sizes along the time (t) and frequency (f) dimensions.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Smoothed array using the boxcar window.\n",
    "\n",
    "    Notes:\n",
    "    - If the input array is 1D, the boxcar window size is adjusted to be odd.\n",
    "    - If the input array is 2D, separate boxcar windows are created for the time (t) and frequency (f) dimensions.\n",
    "\n",
    "    Example:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "\n",
    "    # Example usage:\n",
    "    signal_1d = np.random.rand(100)  # Replace with your actual 1D signal\n",
    "    window_size_1d = 5\n",
    "    smoothed_1d = boxcar_smooth(signal_1d, window_size_1d)\n",
    "    print(smoothed_1d)\n",
    "\n",
    "    signal_2d = np.random.rand(100, 50)  # Replace with your actual 2D signal\n",
    "    window_size_2d = (5, 3)\n",
    "    smoothed_2d = boxcar_smooth(signal_2d, window_size_2d)\n",
    "    print(smoothed_2d)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    # Check if the input array x is 1-dimensional\n",
    "    if x.ndim == 1:\n",
    "        # Check if the boxcar window size is even, and if so, make it odd by adding 1\n",
    "        if boxcar_window % 2 == 0:\n",
    "            boxcar_window += 1\n",
    "        # Create a boxcar window of size boxcar_window for smoothing\n",
    "        window = np.ones((1, boxcar_window)) / boxcar_window\n",
    "        # Perform 1-dimensional convolution to smooth the input array x\n",
    "        x_spectrum = np.convolve(x, window, mode='same')\n",
    "    else:\n",
    "        # Adjust the boxcar window size to be odd for both dimensions\n",
    "        bool_window = np.where(~boxcar_window % 2 == 0, boxcar_window, boxcar_window + 1)\n",
    "        # Create separate boxcar windows for time (t) and frequency (f) dimensions\n",
    "        window_t = np.ones((1, bool_window[0])) / bool_window[0]\n",
    "        window_f = np.ones((1, bool_window[1])) / bool_window[1]\n",
    "        # Perform 2-dimensional convolution first along the time dimension (t)\n",
    "        x_spectrum_t = convolve2d(x, window_t, mode='same')\n",
    "        # Perform 2-dimensional convolution along the frequency dimension (f)\n",
    "        x_spectrum = convolve2d(x_spectrum_t, window_f.T, mode='same')\n",
    "    # Return the smoothed array x_spectrum\n",
    "    return x_spectrum\n",
    "\n",
    "\n",
    "# def peak_cog(frequencies, angles, amplitudes, ratio):\n",
    "#     def nearest_peaks(frequency, angle, amplitude, ratio):\n",
    "#         peak_indices = peak_local_max(amplitude, min_distance=1, threshold_abs=0)\n",
    "#         cog_f = calculate_cog(frequency, angle, amplitude, ratio)\n",
    "\n",
    "#         if peak_indices.shape[0] == 0:\n",
    "#             cog_peak = cog_f\n",
    "#         else:\n",
    "#             cog_fx = np.array([cog_f[0], cog_f[0] * np.cos(np.deg2rad(cog_f[1] - angle[0])),\n",
    "#                                cog_f[0] * np.sin(np.deg2rad(cog_f[1] - angle[0]))])\n",
    "#             peak_loc = peak_loc = np.empty((peak_indices.shape[0], 4))\n",
    "#             peak_loc[:, [0, 1]] = np.array([frequency[peak_indices.T[0]], angle[peak_indices.T[1]]]).T\n",
    "#             peak_loc[:, 2] = peak_loc[:, 0] * np.cos(np.deg2rad(peak_loc[:, 1] - angle[0]))\n",
    "#             peak_loc[:, 3] = peak_loc[:, 0] * np.sin(np.deg2rad(peak_loc[:, 1] - angle[0]))\n",
    "#             peak_loc = peak_loc[:, [0, 2, 3]]\n",
    "#             distances = np.abs(peak_loc - cog_fx)\n",
    "\n",
    "#             cog_pos = peak_indices[np.argmin(np.linalg.norm(distances, axis=1))]\n",
    "\n",
    "#             cog_peak = np.array([frequency[cog_pos[0]], angle[cog_pos[1]]])\n",
    "\n",
    "#         return cog_peak\n",
    "\n",
    "#     if amplitudes.ndim == 2:\n",
    "#         cog = nearest_peaks(frequencies, angles, amplitudes, ratio)\n",
    "#     elif amplitudes.ndim == 3:\n",
    "#         cog = np.empty((amplitudes.shape[0], 2))\n",
    "#         for i, fpp in enumerate(amplitudes):\n",
    "#             cog[i] = nearest_peaks(frequencies, angles, fpp, ratio)\n",
    "#     return cog\n",
    "\n",
    "\n",
    "# def max_peaks(amplitudes):\n",
    "#     new_fpp = np.zeros(amplitudes.shape)\n",
    "#     if amplitudes.ndim == 2:\n",
    "#         peak_indices = peak_local_max(amplitudes, min_distance=1, threshold_abs=0)\n",
    "#         if peak_indices.shape[0] == 0:\n",
    "#             new_fpp = np.where(amplitudes > 0, amplitudes, 0)\n",
    "#         else:\n",
    "#             new_fpp[peak_indices.T[0], peak_indices.T[1]] = amplitudes[peak_indices.T[0], peak_indices.T[1]]\n",
    "#     elif amplitudes.ndim == 3:\n",
    "#         for i, fpp in enumerate(amplitudes):\n",
    "#             peak_indices = peak_local_max(fpp, min_distance=1, threshold_abs=0)\n",
    "#             if peak_indices.shape[0] == 0:\n",
    "#                 new_fpp[i] = np.where(fpp > 0, fpp, 0)\n",
    "#             else:\n",
    "#                 new_fpp[i, peak_indices.T[0], peak_indices.T[1]] = fpp[peak_indices.T[0], peak_indices.T[1]]\n",
    "#     return new_fpp\n",
    "\n",
    "\n",
    "# def boundary_peaks(amplitudes):\n",
    "#     adjusted_fpp = np.zeros(amplitudes.shape)\n",
    "#     if amplitudes.ndim == 2:\n",
    "#         peak_indices = peak_local_max(amplitudes, min_distance=1, threshold_abs=0)\n",
    "#         if peak_indices.shape[0] == 0:\n",
    "#             adjusted_fpp = np.where(amplitudes > 0, amplitudes, 0)\n",
    "#         else:\n",
    "#             new_fpp = amplitudes[peak_indices.T[0], peak_indices.T[1]]\n",
    "#             maximum = np.max(new_fpp)\n",
    "#             minimum = np.min(new_fpp)\n",
    "#             adjusted_fpp = np.where((amplitudes <= maximum) & (amplitudes >= 0.95*minimum), amplitudes, 0)\n",
    "#     elif amplitudes.ndim == 3:\n",
    "#         for i, fpp in enumerate(amplitudes):\n",
    "#             peak_indices = peak_local_max(fpp, min_distance=1, threshold_abs=0)\n",
    "#             print(peak_indices.shape)\n",
    "#             if peak_indices.shape[0] == 0:\n",
    "#                 adjusted_fpp[i] = np.where(fpp > 0, fpp, 0)\n",
    "#             else:\n",
    "#                 maximum = np.max(fpp[peak_indices.T[0], peak_indices.T[1]])\n",
    "#                 minimum = np.min(fpp[peak_indices.T[0], peak_indices.T[1]])\n",
    "#                 adjusted_fpp[i] = np.where((fpp <= maximum) & (fpp >= 0.95*minimum), fpp, 0)\n",
    "#     return adjusted_fpp\n",
    "\n",
    "\n",
    "def rem_fpp_gen(rem_dict, x, sample_rate, frequencies, angles, ratio, boxcar_window=None, norm='', fpp_method='',\n",
    "                cog_method=''):\n",
    "        \"\"\"\n",
    "    Generate Frequency-Power-Phase (FPP) plots for each REM epoch in the input dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - rem_dict (dict): Dictionary containing information about REM epochs and cycles.\n",
    "    - x (numpy.ndarray): 1D sleep signal.\n",
    "    - sample_rate (int or float): Sampling rate of the sleep signal.\n",
    "    - frequencies (numpy.ndarray): Array of frequency values.\n",
    "    - angles (numpy.ndarray): Array of phase angles (in degrees).\n",
    "    - ratio (float): Threshold ratio for identifying significant amplitudes.\n",
    "    - boxcar_window (int or None): Size of the boxcar window for smoothing (default is None).\n",
    "    - norm (str): Normalization method for the time-frequency power spectrum (default is '').\n",
    "    - fpp_method (str): Method for generating FPP plots (default is '').\n",
    "    - cog_method (str): Method for calculating the center of gravity (CoG) (default is '').\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing FPP plots and CoG information for each REM epoch.\n",
    "\n",
    "    Notes:\n",
    "    - The function processes each REM epoch in the input dictionary, extracting relevant information such as cycles,\n",
    "      time indices, and the sleep signal. It then generates FPP plots based on the time-frequency power spectrum\n",
    "      obtained using the Morlet wavelet. Additional options for smoothing, normalization, FPP generation, and CoG calculation\n",
    "      can be applied based on the specified parameters.\n",
    "\n",
    "    \"\"\"\n",
    "    # Ensure the input array x is 1-dimensional\n",
    "    x = np.squeeze(x)\n",
    "    # Create an empty dictionary to store REM features\n",
    "    cycles_dict = rem_dict\n",
    "    rem_dict = {}\n",
    "    # Create a sub-dictionary to store features for each REM epoch\n",
    "    sub_dict = rem_dict\n",
    "    # Loop through each REM epoch in the input dictionary\n",
    "    for key, value in cycles_dict.items():\n",
    "        print(key)\n",
    "        # Check if the REM epoch has cycle information\n",
    "        if 'Cycles' in value.keys():\n",
    "            # Create a sub-dictionary for the current REM epoch\n",
    "            sub_dict.setdefault(key, {})\n",
    "            # Extract the time indices for the current REM epoch\n",
    "            sub_dict.setdefault(key, {})\n",
    "            # Extract the signal for the current REM epoch\n",
    "            t = value['start-end'].astype(np.int32)\n",
    "            print(t, t[0], t[1])\n",
    "            # Extract the signal for the current REM epoch\n",
    "            sig = x[t[0]:t[1]]\n",
    "            print(sig.shape)\n",
    "            # Generate the time-frequency power spectrum using Morlet wavelet\n",
    "            power = morlet_wt(sig, sample_rate, frequencies, mode='power').astype(np.float32)\n",
    "             # Extract cycle information and adjust indices to match the current REM epoch\n",
    "            cycles = (value['Cycles'][:, [0, -1]] - t[0]).astype(np.int32)\n",
    "            # if boxcar_window is not None:\n",
    "            #     power = boxcar_smooth(power, boxcar_window)\n",
    "            # if norm == 'simple_x':\n",
    "            #     power = power / np.sum(power, axis=0)\n",
    "            # elif norm == 'simple_y':\n",
    "            #     power = power / np.sum(power, axis=1)[:, np.newaxis]\n",
    "            # elif norm == 'zscore_y':\n",
    "            #     power = zscore(power, axis=0)\n",
    "            # elif norm == 'zscore_x':\n",
    "            #     power = zscore(power, axis=1)\n",
    "            # Bin the time-frequency power spectrum to generate FPP (Frequency-Power-Phase) plots\n",
    "            fpp_plots = bin_tf_to_fpp(cycles, power, 19).astype(np.float32)\n",
    "            # Store the FPP plots in the sub-dictionary\n",
    "            sub_dict[key]['FPP_cycles'] = fpp_plots\n",
    "            # if fpp_method == 'max_peaks':\n",
    "            #     fpp_plots = max_peaks(fpp_plots)\n",
    "            #     print(fpp_plots.shape)\n",
    "            # elif fpp_method == 'boundary_peaks':\n",
    "            #     fpp_plots = boundary_peaks(fpp_plots)\n",
    "            # if cog_method == 'nearest':\n",
    "            #     cog = peak_cog(frequencies, angles, fpp_plots, ratio).astype(np.float32)\n",
    "            # else:\n",
    "            #     cog = calculate_cog(frequencies, angles, fpp_plots, ratio).astype(np.float32)\n",
    "            # sub_dict[key]['CoG'] = cog\n",
    "        else:\n",
    "            continue\n",
    "    return rem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ae549c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_10-25-02_Pre-sleep\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_10-25-02_Pre-sleep\\2018-10-31_10-25-02_presleep-states.mat\n",
      "LFP data shape: (6751147, 1)\n",
      "States data shape: (1, 2699)\n",
      "Processing data...\n",
      "(6751147,)\n",
      "(2699,)\n",
      "(0,)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "0.6540532112121582\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_11-15-54_Post_Trial1\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_11-15-54_Post_Trial1\\2018-10-31_11-15-54_post_trial1-states.mat\n",
      "LFP data shape: (6750976, 1)\n",
      "States data shape: (1, 2699)\n",
      "Processing data...\n",
      "(6750976,)\n",
      "(2699,)\n",
      "(4, 2)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "291.0214374065399\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_12-06-31_Post_Trial2\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_12-06-31_Post_Trial2\\2018-10-31_12-06-31_post_trial2-states.mat\n",
      "LFP data shape: (6752171, 1)\n",
      "States data shape: (1, 2700)\n",
      "Processing data...\n",
      "(6752171,)\n",
      "(2700,)\n",
      "(4, 2)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "302.2782917022705\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_12-57-33_Post_Trial3\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_12-57-33_Post_Trial3\\2018-10-31_12-57-33_post_trial3-states.mat\n",
      "LFP data shape: (6750806, 1)\n",
      "States data shape: (1, 2699)\n",
      "Processing data...\n",
      "(6750806,)\n",
      "(2699,)\n",
      "(6, 2)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "400.4900965690613\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_13-48-31_Post_Trial4\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_13-48-31_Post_Trial4\\2018-10-31_13-48-31_post_trial4-states.mat\n",
      "LFP data shape: (6751147, 1)\n",
      "States data shape: (1, 2699)\n",
      "Processing data...\n",
      "(6751147,)\n",
      "(2699,)\n",
      "(4, 2)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "384.05989384651184\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_14-39-12_Post_Trial5\\HPC_100_CH32_0.continuous.mat\n",
      "Loading data from: E:/Donders/11/raw/OD\\2018-10-31_14-39-12_Post_Trial5\\2018-10-31_14-39-12_post_trial5-states.mat\n",
      "LFP data shape: (27002795, 1)\n",
      "States data shape: (1, 10800)\n",
      "Processing data...\n",
      "(27002795,)\n",
      "(10800,)\n",
      "(24, 2)\n",
      "Called get_cycles_data\n",
      "Data processing completed.\n",
      "1176.3512046337128\n"
     ]
    }
   ],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    A class for processing and analyzing data.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir, output_dir):\n",
    "        \"\"\"\n",
    "        Initializes the DataProcessor with the input data directory and output directory.\n",
    "\n",
    "        Parameters:\n",
    "        - data_dir (str): The directory containing input data.\n",
    "        - output_dir (str): The directory for storing processed output data.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir  # Set the input data directory.\n",
    "        self.output_dir = output_dir  # Set the output data directory.\n",
    "\n",
    "    def load_data(self, subfolder):\n",
    "        \"\"\"\n",
    "        Load LFP (Local Field Potential) and states data from the specified subfolder.\n",
    "\n",
    "        Parameters:\n",
    "        - subfolder (str): The subfolder within the data directory containing the data files.\n",
    "\n",
    "        Returns:\n",
    "        Tuple: A tuple containing LFP data and states data.\n",
    "        \"\"\"\n",
    "        # Generate file paths for LFP and states data using glob.\n",
    "        # Check for the merged HPC file first, otherwise, use the regular HPC file.\n",
    "        merged_hpc_files = glob.glob(os.path.join(self.data_dir, subfolder, '*_HPC_merged.mat'))\n",
    "        hpc_files = glob.glob(os.path.join(self.data_dir, subfolder, '*HPC*.continuous*.mat'))\n",
    "\n",
    "        if merged_hpc_files:\n",
    "            # If there are merged HPC files, use the first one.\n",
    "            lfp_file = merged_hpc_files[0]\n",
    "        elif hpc_files:\n",
    "            # If no merged HPC files found, but regular HPC files exist, use the first one.\n",
    "            lfp_file = hpc_files[0]\n",
    "        else:\n",
    "            # If no HPC files are found, raise a FileNotFoundError.\n",
    "            raise FileNotFoundError(\"No HPC files found in the specified subfolder.\")\n",
    "\n",
    "        # Find the file that matches the pattern '*states*' in the specified subfolder.\n",
    "        states_file = glob.glob(os.path.join(self.data_dir, subfolder, '*states*'))[0]\n",
    "\n",
    "        print(\"Loading data from:\", lfp_file)\n",
    "        print(\"Loading data from:\", states_file)\n",
    "\n",
    "        # Load LFP and states data using scipy's loadmat function.\n",
    "        lfp_data = sio.loadmat(lfp_file)['HPC']  # Load LFP data from the MATLAB file.\n",
    "        states_data = sio.loadmat(states_file)['states']  # Load states data.\n",
    "\n",
    "        print(\"LFP data shape:\", lfp_data.shape)\n",
    "        print(\"States data shape:\", states_data.shape)\n",
    "\n",
    "        # Check data format\n",
    "        if not isinstance(states_data, np.ndarray):\n",
    "            raise ValueError(\"States data should be a NumPy array.\")\n",
    "\n",
    "        # Check data dimensions (adjust as needed)\n",
    "        if states_data.ndim != 2:\n",
    "            raise ValueError(\"States data should be a 2D array.\")\n",
    "\n",
    "        # Check data range\n",
    "        min_value = np.min(states_data)\n",
    "        max_value = np.max(states_data)\n",
    "        expected_min = 0  # Adjust as needed\n",
    "        expected_max = 5  # Adjust as needed\n",
    "        if min_value < expected_min or max_value > expected_max:\n",
    "            raise ValueError(\"States data range is outside of expected bounds.\")\n",
    "\n",
    "        # Check for missing values\n",
    "        if np.isnan(states_data).any():\n",
    "            raise ValueError(\"States data contains NaN values.\")\n",
    "\n",
    "        return lfp_data, states_data \n",
    "     \n",
    "\n",
    "    def process_data(self, lfp_data, states_data):\n",
    "        \"\"\"\n",
    "        Process LFP (Local Field Potential) and states data.\n",
    "\n",
    "        Parameters:\n",
    "        - lfp_data (numpy.ndarray): The LFP data to be processed.\n",
    "        - states_data (numpy.ndarray): The states data corresponding to the LFP data.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary containing processed data.\n",
    "        \"\"\"\n",
    "        print(\"Processing data...\")\n",
    "        # Define a frequency range for processing.\n",
    "        frequency_range=np.arange(20,140,1)\n",
    "        # Call a function (get_cycles_data) to process cycles data.\n",
    "        rem_dict = get_cycles_data(lfp_data, states_data, 2500, frequency_range, (5, 12))\n",
    "        print(\"Called get_cycles_data\")\n",
    "        for key, value in rem_dict.items():\n",
    "            # Loop through each key-value pair in rem_dict.\n",
    "            if isinstance(value, (int, float)):\n",
    "                # Check if the value associated with the key is an int or float.\n",
    "                rem_dict[key] = np.float32(value)  # Convert numerical values to np.float32\n",
    "        # Print a message indicating that data processing is completed.\n",
    "        print(\"Data processing completed.\")\n",
    "        # Return the processed rem_dict.\n",
    "        return rem_dict\n",
    "\n",
    "    \n",
    "    def save_data(self, subfolder, rem_dict):\n",
    "        \"\"\"\n",
    "        Save processed data to a file.\n",
    "\n",
    "        Parameters:\n",
    "        - subfolder (str): The subfolder within the output directory for saving data.\n",
    "        - rem_dict (dict): The processed data dictionary to be saved.\n",
    "        \"\"\"\n",
    "        # Create an output subfolder if it doesn't exist.\n",
    "        output_subfolder = os.path.join(self.output_dir, subfolder)\n",
    "        # Create a new folder in the output directory to store the processed data.\n",
    "        os.makedirs(output_subfolder, exist_ok=True)\n",
    "        # Define the output file paths based on the subfolder name.\n",
    "        rem_dict_filename = f\"{subfolder}_REM_dict.h5\"\n",
    "        # Create the full file path for saving the processed data dictionary.\n",
    "        rem_dict_file = os.path.join(output_subfolder, rem_dict_filename)\n",
    "\n",
    "        # Create a function to save a dictionary as an HDF5 group\n",
    "        def save_dict_as_hdf5_group(hdf_group, data_dict):\n",
    "            \"\"\"\n",
    "            Recursively saves a dictionary as an HDF5 group.\n",
    "\n",
    "            Parameters:\n",
    "            - hdf_group (h5py.Group): The HDF5 group to which the dictionary will be saved.\n",
    "            - data_dict (dict): The dictionary to be saved.\n",
    "            \"\"\"\n",
    "            \n",
    "            for key, value in data_dict.items():\n",
    "                # Loop through each key-value pair in the dictionary.\n",
    "                if isinstance(value, dict):\n",
    "                    # If the value is another dictionary, create a subgroup in the HDF5 group.\n",
    "                    subgroup = hdf_group.create_group(key)\n",
    "                    # Create a subgroup within the HDF5 group.\n",
    "                    save_dict_as_hdf5_group(subgroup, value)\n",
    "                else:\n",
    "                    # Otherwise, save the value to the HDF5 group\n",
    "                    hdf_group[key] = value\n",
    "\n",
    "        # Save the rem_dict dictionary as an HDF5 file.\n",
    "        with h5py.File(rem_dict_file, 'w') as hdf_file:\n",
    "            # Use the subfolder name as the top-level group name\n",
    "            subfolder_group = hdf_file.create_group(subfolder)\n",
    "\n",
    "            # Call the function to save rem_dict within the subfolder group\n",
    "            save_dict_as_hdf5_group(subfolder_group, rem_dict)\n",
    "\n",
    "    def process_subfolder_with_timing(self, subfolder):\n",
    "        \"\"\"\n",
    "        Process a subfolder's data with timing information.\n",
    "\n",
    "        Parameters:\n",
    "        - subfolder (str): The subfolder within the data directory to process.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The processed data dictionary (rem_dict).\n",
    "        \"\"\"\n",
    "        start_time = time.time()  # Start measuring time\n",
    "        # Load LFP (Local Field Potential) and states data from the specified subfolder.\n",
    "        lfp_data, states_data = self.load_data(subfolder)\n",
    "        # Process the loaded data and obtain the rem_dict (processed data dictionary).\n",
    "        rem_dict = self.process_data(lfp_data, states_data)  # Process and get the rem_dict\n",
    "        # Save the processed data dictionary to the output directory.\n",
    "        self.save_data(subfolder, rem_dict)  # Save the rem_dict\n",
    "        end_time = time.time()  # Stop measuring time\n",
    "        elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "        print(elapsed_time)  # Print the elapsed time in seconds\n",
    "        # Return the processed data dictionary (rem_dict).\n",
    "        return rem_dict\n",
    "\n",
    "\n",
    "\n",
    "# Specify the input data directory and output directory.\n",
    "data_dir = \"E:/Donders/11/raw/OD\"  # Replace with your actual data directory path.\n",
    "output_dir = \"E:/Donders/11/processed/OD\"  # Replace with your desired output directory path.\n",
    "\n",
    "# DataProcessor object.\n",
    "processor = DataProcessor(data_dir, output_dir)\n",
    "\n",
    "# Process all data in the specified directories using parallel processing.\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    \"\"\"\n",
    "    This code concurrently processes data from multiple subfolders using a concurrent.futures.ProcessPoolExecutor. \n",
    "    \n",
    "    It starts by creating the executor, then generates a list of subfolders in the specified data directory.\n",
    "    \n",
    "    In a parallel loop, it calls the process_subfolder_with_timing method of a DataProcessor object for each subfolder, \n",
    "    collecting the processed data dictionaries in the processed_data list. The result is a list containing the \n",
    "    dictionaries for each subfolder after parallel processing.\n",
    "    \"\"\"\n",
    "    # Get a list of subfolders within the specified data directory.\n",
    "    subfolders = [subfolder for subfolder in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subfolder))]\n",
    "    # Create a list to collect processed data dictionaries for each subfolder.\n",
    "    processed_data = []  # Create a list to collect processed data\n",
    "    # Iterate over each subfolder for parallel processing.\n",
    "    for subfolder in subfolders:\n",
    "        # Call the 'process_subfolder_with_timing' method on the DataProcessor object for each subfolder.\n",
    "        result = processor.process_subfolder_with_timing(subfolder)  # Call the method and collect the dictionaries\n",
    "        # Append the processed data dictionary to the list.\n",
    "        processed_data.append(result)  # Append the dictionaries to the list\n",
    "\n",
    "# Now, 'processed_data' contains the dictionaries for each subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d69c858",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing HDF file: post_trial1_2017-09-28_11-30-59_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial1_2017-09-28_11-30-59\n",
      "Processing HDF file: post_trial1_2017-11-13_10-54-15_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial1_2017-11-13_10-54-15\n",
      "Processing HDF file: post_trial2_2017-09-28_12-21-41_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial2_2017-09-28_12-21-41\n",
      "Processing HDF file: post_trial2_2017-11-13_11-45-00_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial2_2017-11-13_11-45-00\n",
      "Processing HDF file: post_trial3_2017-09-28_13-12-53_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial3_2017-09-28_13-12-53\n",
      "Processing HDF file: post_trial3_2017-11-13_12-35-45_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial3_2017-11-13_12-35-45\n",
      "Processing HDF file: post_trial4_2017-09-28_14-03-38_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial4_2017-09-28_14-03-38\n",
      "Processing HDF file: post_trial4_2017-11-13_13-26-21_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial4_2017-11-13_13-26-21\n",
      "Processing HDF file: post_trial5_2017-09-28_14-55-18_REM_dict.h5\n",
      "Keys in HDF file:\n",
      "post_trial5_2017-09-28_14-55-18\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd6d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5412500. 5625000.]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e498da3",
   "metadata": {},
   "source": [
    "print(rem_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07087d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa7a5fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d8fb1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(2,)\n",
      "5412500.0\n",
      "<class 'int'>\n",
      "Processing REM period 1, start: 5412500, end: 5625000\n",
      "start: 5412500, end: 5625000\n",
      "start: 5412500, end: 5625000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (613,5) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rem_dict\u001b[38;5;241m=\u001b[39m\u001b[43mget_cycles_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHPC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [33], line 157\u001b[0m, in \u001b[0;36mget_cycles_data\u001b[1;34m(x, rem_states, sample_rate, theta_range)\u001b[0m\n\u001b[0;32m    155\u001b[0m rem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstantaneous Frequencies\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m instantaneous_freq[j]\n\u001b[0;32m    156\u001b[0m rem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstantaneous Amplitudes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m instantaneous_amp[j]\n\u001b[1;32m--> 157\u001b[0m cycles_mask \u001b[38;5;241m=\u001b[39m (\u001b[43mcycles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconsecutive_rem_states\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;241m&\u001b[39m (cycles \u001b[38;5;241m<\u001b[39m consecutive_rem_states[j, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    158\u001b[0m cycles_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(cycles_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    159\u001b[0m rem_cycles \u001b[38;5;241m=\u001b[39m cycles[cycles_mask]\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (613,5) (2,) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d8515c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66e8fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
